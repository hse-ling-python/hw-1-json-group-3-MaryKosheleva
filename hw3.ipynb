{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Читаю файл с твитами и разбиваю его на строки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "twitter = []\n",
    "for line in open('hw_3_twitter.json'):\n",
    "    twitter.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти количество твитов в наборе, нахожу длину получившегося файла, так как одна строка - один твит."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: количество твитов в наборе - 2556 твитов.\n"
     ]
    }
   ],
   "source": [
    "all_tweets = len(twitter)\n",
    "print('Ответ: количество твитов в наборе -', all_tweets, 'твитов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти количество удалённых твитов, вытаскиваю список ключей каждого твита и проверяю первый элемент на совпадение со строкой \"delete\", так как \"delete\" - первый ключ удалённых твитов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: процент удалённых твитов - 14.162754303599373 процентов.\n"
     ]
    }
   ],
   "source": [
    "deleted_counter = 0\n",
    "\n",
    "for line in twitter:\n",
    "    keys = list(line.keys())\n",
    "    if keys[0] == 'delete':\n",
    "        deleted_counter += 1\n",
    "\n",
    "# делю кол-во удалённых твитов на кол-во всех твитов\n",
    "deleted_percent = deleted_counter / len(twitter) * 100\n",
    "\n",
    "print('Ответ: процент удалённых твитов -', deleted_percent, 'процентов.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция принимает на вход частотный словарь и число, задающее количество элементов, которое нужно вывести. Функция превращает словарь в отсортированный список из кортежей, расположенных в порядке убывания по частоте, и выводит заданное количество первых элементов. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_dict_max(all_dict, number):\n",
    "    all_dict_sorted = sorted(all_dict.items(), key=lambda x:\n",
    "                             (x[1], x[0]), reverse=True)\n",
    "    counter = 0\n",
    "    for item in all_dict_sorted:\n",
    "        print(item[0], ' : ', item[1])\n",
    "        counter += 1\n",
    "        if counter >= number:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти топ-16 самых популярных языков твитов, создаю словарь и прохожу по каждому твиту (ключ - \"lang\"), считывая его язык. Если в словаре ещё не существует элемент с таким ключом (язык твита), то создаю новый элемент с языком твита в ключе и количеством употребления в значении. Увеличиваю значение элемента на 1. Затем применяю функцию sort_dict_max(), чтобы найти топ-16 языков твитов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: топ-16 языков твитов:\n",
      "en  :  719\n",
      "ja  :  438\n",
      "es  :  173\n",
      "ko  :  149\n",
      "th  :  123\n",
      "ar  :  119\n",
      "und  :  117\n",
      "in  :  71\n",
      "pt  :  69\n",
      "fr  :  35\n",
      "tr  :  30\n",
      "tl  :  29\n",
      "hi  :  23\n",
      "ru  :  15\n",
      "zh  :  8\n",
      "fa  :  8\n"
     ]
    }
   ],
   "source": [
    "tweet_lang = {}\n",
    "\n",
    "for line in twitter:\n",
    "    keys = list(line.keys())\n",
    "    if keys[0] != 'delete':  # проверяю твит на существование\n",
    "        tweet_lang.setdefault(line['lang'], 0)\n",
    "        tweet_lang[line['lang']] += 1\n",
    "\n",
    "print('Ответ: топ-16 языков твитов:')\n",
    "sort_dict_max(tweet_lang, 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти количество пользователей, которым принадлежат несколько твитов из набора (ключ - \"id_str\"), создаю словарь, в котором лежат элементы с id пользователя в ключе и с количеством встретившихся твитов этого пользователя в значении. Алгоритм такой же, как в пункте 3. Затем прохожу по словарю и складываю количество пользователей, имеющих больше одного твита в наборе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: количество пользователей с несколькими твитами в наборе - 25 пользователей.\n"
     ]
    }
   ],
   "source": [
    "users = {}\n",
    "\n",
    "for line in twitter:\n",
    "    keys = list(line.keys())\n",
    "    if keys[0] != 'delete':  # проверяю твит на существование\n",
    "        user_id = line['user']['id_str']\n",
    "        users.setdefault(user_id, 0)\n",
    "        users[user_id] += 1\n",
    "\n",
    "users_counter = 0\n",
    "\n",
    "for number in list(users.values()):\n",
    "    if number > 1:\n",
    "        users_counter += 1\n",
    "\n",
    "print('Ответ: количество пользователей с несколькими твитами в наборе -',\n",
    "      users_counter, 'пользователей.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти топ-20 хэштегов, создаю список элементов, в который добавляю каждый хэштег, находящийся под ключом \"text\", который, в свою очередь, находится под ключами \"entities\" и \"hashtags\". Затем с помощью Counter создаю частотный словарь. Далее применяю функцию sort_dict_max(), чтобы найти топ-20 хэштегов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: топ-20 хэштегов:\n",
      "BTS  :  17\n",
      "방탄소년단  :  13\n",
      "AMAs  :  11\n",
      "人気投票ガチャ  :  8\n",
      "태형  :  7\n",
      "뷔  :  6\n",
      "오늘의방탄  :  5\n",
      "PledgeForSwachhBharat  :  5\n",
      "MPN  :  5\n",
      "BTSinChicago  :  5\n",
      "BTSLoveYourselfTour  :  5\n",
      "시카고1회차공연  :  4\n",
      "เป๊กผลิตโชค  :  4\n",
      "V  :  4\n",
      "PCAs  :  4\n",
      "JIMIN  :  4\n",
      "지민  :  3\n",
      "running  :  3\n",
      "WajahmuPlastik  :  3\n",
      "NCT127  :  3\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "hashtags_list = []\n",
    "\n",
    "for line in twitter:\n",
    "    keys = list(line.keys())\n",
    "    if keys[0] != 'delete':  # проверяю твит на существование\n",
    "        for hashtag in line['entities']['hashtags']:\n",
    "            hashtags_list.append(hashtag['text'])\n",
    "\n",
    "hashtags = Counter(hashtags_list)\n",
    "\n",
    "print('Ответ: топ-20 хэштегов:')\n",
    "sort_dict_max(hashtags, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы составить частотный словарь слов оригинальных твитов на английском языке, построчно прохожу по файлу и ищу твиты, которые не имеют ключи \"retweeted_status\" и \"qouted_status\" (так как они свидетельствую о неоригинальности твита), а также \"delete\", но языком которых является английский. Вынимаю текст твита по ключу \"text\" и очищаю его от знаков препинания, привожу к нижнему регистру. Разбиваю очищенный текст на слова и состовляю частотный словарь, элементы которого отсортировываю функцией sort_dict_max()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: топ-20 слов из частотного словаря ориг. твитов на англ. языке.:\n",
      "the  :  107\n",
      "to  :  79\n",
      "a  :  68\n",
      "i  :  60\n",
      "and  :  58\n",
      "you  :  45\n",
      "of  :  41\n",
      "is  :  41\n",
      "for  :  40\n",
      "it  :  38\n",
      "in  :  32\n",
      "that  :  30\n",
      "my  :  26\n",
      "on  :  25\n",
      "me  :  25\n",
      "be  :  22\n",
      "this  :  20\n",
      "are  :  20\n",
      "have  :  19\n",
      "so  :  18\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "words = []\n",
    "\n",
    "# символы, от которых надо очистить текст\n",
    "punctuation_extended = list(string.punctuation) + ['…', '°']\n",
    "punctuation_extended.extend('0123456789')\n",
    "\n",
    "for line in twitter:\n",
    "    keys = list(line.keys())\n",
    "    if ('retweeted_status' not in keys and 'quoted_status' not in keys and\n",
    "            keys[0] != 'delete'):\n",
    "        if line['lang'] == 'en':\n",
    "            tweet_text = line['text'].lower()\n",
    "            for char in punctuation_extended:\n",
    "                tweet_text = tweet_text.replace(char, '')\n",
    "            tweet_list = tweet_text.split()\n",
    "            words.extend(tweet_list)\n",
    "\n",
    "words_counter = Counter(words)\n",
    "\n",
    "print('Ответ: топ-20 слов из частотного словаря ориг. твитов на англ. языке.:')\n",
    "sort_dict_max(words_counter, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти топ-10 авторов твитов с наибольшим количеством подписчиков, создаю словарь с элементами, ключом которых является id пользователя, а значением - количество его подписчиков. Затем применяю функцию sort_dict_max() и вывожу топ-10 пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: топ-10 пользоват. с наибольшим кол-вом подписчиков       (id : кол-во):\n",
      "262389559  :  2521403\n",
      "14562016  :  1491309\n",
      "18040230  :  1206759\n",
      "1435461  :  1137374\n",
      "290401936  :  625463\n",
      "234419133  :  392472\n",
      "20689749  :  383698\n",
      "223282435  :  374222\n",
      "634594307  :  318189\n",
      "399622781  :  311319\n"
     ]
    }
   ],
   "source": [
    "followers_total = {}\n",
    "\n",
    "for line in twitter:\n",
    "    keys = list(line.keys())\n",
    "    if keys[0] != 'delete':  # проверяю твит на существование\n",
    "        user_id = line['user']['id_str']\n",
    "        followers_count = line['user']['followers_count']\n",
    "        followers_total[user_id] = followers_count\n",
    "\n",
    "print('Ответ: топ-10 пользоват. с наибольшим кол-вом подписчиков \\\n",
    "      (id : кол-во):')\n",
    "sort_dict_max(followers_total, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пункт 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы найти топ-10 источников твита, прохожу по строкам с твитами и по ключу \"source\" нахожу источник твита, который нужно вынуть с помощью регулярных выражений. Из списка составляю частотный словарь, который потом обрабатываю функцией sort_dict_max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ответ: топ-10 источников твита:\n",
      "Twitter for iPhone  :  800\n",
      "Twitter for Android  :  695\n",
      "Twitter Web Client  :  140\n",
      "twittbot.net  :  122\n",
      "Twitter Lite  :  51\n",
      "Twitter for iPad  :  28\n",
      "TweetDeck  :  23\n",
      "Facebook  :  17\n",
      "IFTTT  :  14\n",
      "تطبيق قرآني  :  10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sources = []\n",
    "\n",
    "for line in twitter:\n",
    "    keys = list(line.keys())\n",
    "    if keys[0] != 'delete':  # проверяю твит на существование\n",
    "        source = line['source']\n",
    "        source_bare = re.search('rel=\"nofollow\">([^<]*)</a>', source)\n",
    "        sources.append(source_bare.group(1))\n",
    "\n",
    "sources_counter = Counter(sources)\n",
    "\n",
    "print('Ответ: топ-10 источников твита:')\n",
    "sort_dict_max(sources_counter, 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
